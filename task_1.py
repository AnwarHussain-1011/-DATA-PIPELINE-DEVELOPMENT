# -*- coding: utf-8 -*-
"""TASK-1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LkCkHhE1_gyDfTxCrYGlNQTCfhJNsA9z
"""

# task1_data_pipeline.py

import pandas as pd
from sklearn import datasets
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np
from scipy.stats import zscore

# Step 1: Load Dataset (Iris Dataset)
iris = datasets.load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
data['target'] = iris.target
data['species'] = iris.target_names[iris.target]

print("Original Data Head:")
print(data.head())

# Step 2: Handle Missing Values (Simulated)
data.loc[5:10, 'sepal length (cm)'] = np.nan
print("\nMissing values before filling:")
print(data.isnull().sum())

data['sepal length (cm)'].fillna(data['sepal length (cm)'].mean(), inplace=True)

# Step 3: Outlier Detection (Z-score method)
z_scores = np.abs(zscore(data.select_dtypes(include=[np.number])))
outliers = (z_scores > 3)
print("\nNumber of outliers (Z-score > 3):")
print(outliers.sum())

# Step 4: Encode Categorical Data
label_encoder = LabelEncoder()
data['species_encoded'] = label_encoder.fit_transform(data['species'])

# Step 5: Feature Scaling
features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data[features])

scaled_df = pd.DataFrame(data_scaled, columns=[f"{col}_scaled" for col in features])
data = pd.concat([data.reset_index(drop=True), scaled_df], axis=1)

# Step 6: Save Cleaned Data
data.to_csv("cleaned_iris_data.csv", index=False)
print("\nCleaned data saved to 'cleaned_iris_data.csv'")